#!/usr/bin/env python3

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt   
from scipy import stats
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm

# set font to Arial
import matplotlib as mpl
mpl.rc('font',family='Arial')

# datasets can be found in accompanying Zenodo repository

all_corrs = np.load('all_corrs.npy') # load daily pattern correlations file

dates = pd.date_range(start='1/1/1940', end='12/31/2023')
dates = dates[~((dates.month == 2) & (dates.day == 29))]
dates = pd.DataFrame(dates)
dates['month'] = pd.to_datetime(dates.iloc[:,0]).dt.month
dates = dates[dates.month.isin([4,5,6,7,8,9])]
dates.reset_index(inplace=True)

day_idx = np.arange(1,184,1)
day_idxs = np.tile(day_idx,83)

# match patterns for each day during 14-day heatwave period (June 15-28, 2023)
for m in range(14):
    corrs = all_corrs[15264+m,:] # starting at day 15264 = 6/15/2023

    df1 = pd.DataFrame({'date':dates.iloc[:,1],'month':dates.month,
                        'corr':corrs})  

    df1['year'] = pd.to_datetime(df1.date).dt.year
    df2 = df1[df1.year<=2022]
    df2['day_idx'] = day_idxs
    df3 = df2[df2.day_idx.between(61+m,91+m)]

    dfx = df3.sort_values(by='corr',ascending=False)
    day_accum = []
    for k in range(df3.shape[0]):
        day = dfx.index.values[k]
        window = list(np.arange(day-3,day+4,1)) # 7-day window
        i, j = window[0], window[-1]
        res = any(ele >= i and ele <= j for ele in day_accum)
        if res == 0:
            day_accum.append(day)
    day_accum = np.array(day_accum)

    df5 = df3.loc[day_accum]
    df8 = df5.iloc[:40] # grab top-40

    mats_idx = df8.index.values

    np.save(f'mats_idx{m}',mats_idx) # save the index of matched days for each of the 14 days during heatwave

# load domain-averaged variables
z500_amjjas2_z = np.load('zvec_smaller.npy')
z500_amjjas2_z = np.nanmax(z500_amjjas2_z,axis=1) # using max of detrended Z500 z-scores in smaller rectangle as predictor

tmax_detrended_z = np.load('tmax_detrended_z.npy')
tmax_z_normed2 = np.load('tmax_z_normed2.npy')
sm_z_normed2 = np.load('sm_z_normed2.npy')

dates = pd.date_range(start='1/1/1940', end='12/31/2023')
dates = dates[~((dates.month == 2) & (dates.day == 29))]
dates = pd.DataFrame(dates)
dates['month'] = pd.to_datetime(dates.iloc[:,0]).dt.month
dates = dates[dates.month.isin([4,5,6,7,8,9])]
idx = dates.index.values

# run predictive models (simple and MLR) for analogs

actuals = []
preds = []
preds_mlr = []
for m in range(14):
    mats_idx = np.load(f'mats_idx{m}.npy') # for each of the 14 heatwave days, load the index of top-40 matched patterns (these were generated by previous code block above)
    
    zv = z500_amjjas2_z
    z = zv[mats_idx]
    tv = tmax_detrended_z # detrended tmax z-scores to train the models
    t = tv[mats_idx]
    
    day = zv[15264+m]
    day2 = tmax_z_normed2[15264+m] # actual z-scores (not detrended)
    actuals.append(day2)
    
    dfx = pd.DataFrame({'z':z,'t':t})
    dfx.dropna(inplace=True)
    X = np.array(dfx.z)
    y = np.array(dfx.t)
    
    X = X.reshape(-1,1)
    y = y.reshape(-1,1)
    reg = LinearRegression().fit(X, y) # fit univariate OLS model

    new_val = np.array(day)
    new_val = new_val.reshape(-1,1)
    preds.append(np.float64(reg.predict(new_val))) # predict domain-averaged Tmax using OLS model
    
    # MLR models
    
    s = sm_z_normed2[mats_idx] # bringin in SM as 2nd predictor
    
    predict_df = pd.DataFrame({'z500':z,
                               'SM':s})
    X = predict_df
    y = pd.DataFrame({'tmax':t})

    model = sm.OLS(y, X).fit() # fitting MLR model using statsmodels package
    
    predict_2023 = np.array([zv[15264+m],sm_z_normed2[15264+m]])
    X_2023 = predict_2023
    preds_mlr.append(np.float64(model.predict(X_2023))) # predict domain-averaged Tmax using MLR model

preds = np.array(preds) # predictions of detrended Tmax z-scores

# first, compute the detrended Tmax values that these z-scores represent
daily_std_detrended = np.load('daily_std_detrended.npy')
std_deg_C = daily_std_detrended[15264:15278]
dep_deg_C = std_deg_C * preds
daily_avg_detrended = np.load('daily_avg_detrended.npy')
pred_deg_C = daily_avg_detrended[15264:15278] + dep_deg_C

# add Tmax trend back in
trend_increments = np.load('tmax_trend_increments.npy')
this_increment = trend_increments[-1]
preds_trend_added = pred_deg_C + this_increment

tmax_normed = np.load('tmax_normed.npy') 
tmax_normed_domain = np.nanmean(tmax_normed,axis=1) # observed domain-average Tmax in actual units (Deg C), latitude normalized
observed_tmax = tmax_normed_domain[15264:15278]

# tie % explained to anomalies in deg C, so they are directly interpretable from panel A
averages = daily_avg_detrended[15264:15278]
full_deps = observed_tmax - averages
perc_explained_circulation = ((pred_deg_C-averages)/full_deps)*100
perc_explained_circulation = np.where(perc_explained_circulation<0,0,perc_explained_circulation)
perc_explained_trend_added = ((preds_trend_added-averages)/full_deps)*100

# repeat the above workflow for MLR

preds_mlr = np.array(preds_mlr)

# first, compute the detrended Tmax values that these z-scores represent
daily_std_detrended = np.load('daily_std_detrended.npy')
std_deg_C = daily_std_detrended[15264:15278]
dep_deg_C_mlr = std_deg_C * preds_mlr
daily_avg_detrended = np.load('daily_avg_detrended.npy')
pred_deg_C_mlr = daily_avg_detrended[15264:15278] + dep_deg_C_mlr

# add Tmax trend back in
trend_increments = np.load('tmax_trend_increments.npy')
this_increment = trend_increments[-1]
preds_trend_added_mlr = pred_deg_C_mlr + this_increment

# tie % explained to anomalies in deg C, so they are directly interpretable from panel A
averages = daily_avg_detrended[15264:15278]
full_deps = observed_tmax - averages
perc_explained_circulation_mlr = ((pred_deg_C_mlr-averages)/full_deps)*100
perc_explained_circulation_mlr = np.where(perc_explained_circulation_mlr<0,0,perc_explained_circulation_mlr)
perc_explained_trend_added_mlr = ((preds_trend_added_mlr-averages)/full_deps)*100

# plot fig 17a

fig, ax1 = plt.subplots(figsize=(8,6))
ax1.set_ylabel('Domain-averaged Tmax ($^\circ$C)', fontsize = 16)
ax1.set_yticks(np.arange(30,40.01,1))
ax1.set_ylim(top=40)
ax1.set_ylim(bottom=30)
ax1.set_xticks([0,3,6,9,12])
ax1.set_xticklabels(['June 15','June 18','June 21','June 24','June 27'], fontsize = 13)
ax1.plot(observed_tmax,'-o', color='firebrick', lw=2)
ax1.plot(pred_deg_C,'-o', color='0.5', lw=2)
ax1.plot(preds_trend_added,'-o', color='k', lw=2)
ax1.plot(preds_trend_added_mlr,'-o', color='cornflowerblue', lw=2)
ax1.tick_params(axis='y', labelsize=13)
plt.title('June 15-28, 2023 daily domain-averaged \n Tmax and analog Tmax', fontsize = 20)
plt.savefig('fig_4a.pdf',dpi=600)

# re-plot this on extended y-axis to make legend
fig = plt.figure(figsize=(8,6))
plt.plot(observed_tmax,'-o', color='firebrick', lw=2)
plt.plot(preds_trend_added_mlr,'-o', color='cornflowerblue', lw=2)
plt.plot(preds_trend_added,'-o', color='k', lw=2)
plt.plot(pred_deg_C,'-o', color='0.5', lw=2)
plt.yticks(np.arange(-5000,6001,1000), fontsize=13)
l = plt.legend(['June 15-28, 2023 Tmax',
            'Tmax predicted by Z500 and SM + Tmax trend',
            'Tmax predicted by Z500 + Tmax trend',
            'Tmax predicted by Z500 only'], 
            loc='lower right', fontsize=15)
colors = ['firebrick','cornflowerblue','k','0.5']
n = -1
for text in l.get_texts():
    n += 1
    text.set_color(colors[n])
plt.savefig('fig_4a_legend.pdf',dpi=600)

# plot fig 17b

fig, ax2 = plt.subplots(figsize=(8,6))
color = 'tab:brown'
ax2.set_ylabel('% of +Tmax anomalies explained', color='k', fontsize=16)  
ax2.set_yticks(np.arange(0,101,10))
ax2.set_ylim(top=102)
ax2.set_ylim(bottom=-2)
ax2.set_xticks([0,3,6,9,12])
ax2.set_xticklabels(['June 15','June 18','June 21','June 24','June 27'], fontsize = 13)
ax2.plot(perc_explained_circulation, '-o', color='0.5', lw=2)
ax2.plot(perc_explained_trend_added, '-o', color='k', lw=2)
ax2.plot(perc_explained_trend_added_mlr, '-o', color='cornflowerblue', lw=2)
ax2.tick_params(axis='y', labelcolor='k')
ax2.tick_params(axis='y', labelsize=13)
plt.title('June 15-28, 2023 percent of \n +Tmax anomalies explained by analogs', fontsize = 20)
plt.show()
plt.savefig('fig_4b.pdf',dpi=600)

# re-plot this on extended y-axis to make legend
fig = plt.figure(figsize=(8,6))
plt.plot(perc_explained_trend_added_mlr, '-o', color='cornflowerblue', lw=2)
plt.plot(perc_explained_trend_added, '-o', color='k', lw=2)
plt.plot(perc_explained_circulation, '-o', color='0.5', lw=2)
plt.yticks(np.arange(-5000,6001,1000), fontsize=13)
l = plt.legend(['% explained by Z500 and SM + Tmax trend',
                '% explained by Z500 + Tmax trend',
                '% explained by Z500 only'],
               loc='lower right', fontsize=15)
colors = ['cornflowerblue','k','0.5']
n = -1
for text in l.get_texts():
    n += 1
    text.set_color(colors[n])
plt.savefig('fig_4b_legend.pdf',dpi=600)

# plot figure 18a

# soil moisture on the analog days
sm_2023 = sm_z_normed2[15264:15278]
sm_means = []
for m in range(14):
    mats_idx = np.load(f'mats_idx{m}.npy')
    sm_means.append(np.mean(sm_z_normed[mats_idx]))

# precip on the analog days
prec_normed = np.load('prec_normed.npy') # load latitude-normalized precipitation data 
prec_normed_means = np.nanmean(prec_normed,axis=1) # compute domain averages

prec_2023 = prec_normed_means[30460:30474] # the precipitation dataset covers full annual cycle rather than April-Sept, so it's indexed differently

dates = pd.date_range(start='1/1/1940', end='12/31/2023')
dates = dates[~((dates.month == 2) & (dates.day == 29))]
dates = pd.DataFrame(dates)
dates['month'] = pd.to_datetime(dates.iloc[:,0]).dt.month
dates = dates[dates.month.isin([4,5,6,7,8,9])]
dates.reset_index(inplace=True)

prec_means = []
for m in range(14):
    mats_idx = np.load(f'mats_idx{m}.npy')
    allyear_idx = np.array(dates.loc[mats_idx].iloc[:,0])
    prec_means.append(np.nanmean(prec_normed_means[mats_idx]))

fig, ax1 = plt.subplots(figsize=(8,6))
color = 'tab:brown'
#plt.grid(linewidth=0.5, color='0.5', alpha=0.5, linestyle='-.')
ax1.set_ylabel('Domain-averaged SM standardized anomaly', fontsize = 16, color = color)
ax1.set_yticks(np.arange(-3,1.01,0.5))
ax1.set_ylim(top=1)
ax1.set_ylim(bottom=-3)
ax1.set_xticks([0,3,6,9,12])
ax1.set_xticklabels(['June 15','June 18','June 21','June 24','June 27'], fontsize = 13)
ax1.plot(sm_2023,'-o', color=color, lw=2)
ax1.plot(sm_means,'--', color=color, lw=1)
ax1.tick_params(axis='y', labelsize=13, labelcolor=color)
plt.title('June 15-28, 2023 daily domain-averaged \n SM and precipitation', fontsize = 20)
ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis
color = 'tab:green'
ax2.set_ylabel('Domain-averaged precip (mm)', color=color, fontsize=16)  
ax2.set_yticks(np.arange(0,4.01,0.5))
ax2.set_ylim(top=4)
ax2.set_ylim(bottom=0)
ax2.plot(prec_2023, '-o', color=color, lw=2)
ax2.plot(prec_means, '--', color=color, lw=1)
ax2.tick_params(axis='y', labelsize=13, labelcolor=color)

# re-plot this on extended y-axis to make legend
fig = plt.figure(figsize=(8,6))
plt.plot(sm_2023,'-o', color='tab:brown', lw=2)
plt.plot(sm_means,'--', color='tab:brown', lw=1)
plt.plot(prec_2023, '-o', color='tab:green', lw=2)
plt.plot(prec_means, '--', color='tab:green', lw=1)
plt.yticks(np.arange(-5000,6001,1000), fontsize=13)
l = plt.legend(['June 15-28, 2023 SM',
            '1940-2022 analog SM',
            'June 15-28, 2023 Prec',
            '1940-2022 analog Prec'], 
            loc='lower right', fontsize=15)
for text in l.get_texts():
    s = str(text)
    print(s)
    if s[29] == 'S':
        text.set_color('tab:brown')
    else:
        text.set_color('tab:green')

# plot figure 18b

prec_30d_domain = np.load('prec_30d_domain.npy') # dataset of lagged 30-day precip totals for each date, domain averaged
prec_30d_domain = (100-prec_30d_domain) * -1

var = prec_30d_domain[15269,:]
lat = np.arange(14.75,36.76,0.25)
lon = np.arange(-118.5,-86.74,0.25)
lon, lat = np.meshgrid(lon, lat)

plt.figure(figsize=(10, 8))
m = Basemap(projection='lcc', area_thresh=10000, resolution='l',
            width=3.3E6, height=3.3E6,
            lat_0=25, lon_0=-102)
m.fillcontinents(color='white',lake_color='white',alpha=1,zorder=0)
m.drawstates(linewidth=0.5,linestyle='solid',color='k')
m.drawcountries(linewidth=0.5,linestyle='solid',color='k')
m.drawcoastlines(color='black')
m.pcolormesh(lon, lat, var, latlon=True, cmap=plt.cm.get_cmap('BrBG',20))
cb = plt.colorbar(ticks=np.arange(-100,101,20),extend='max')
cb.ax.tick_params(labelsize=15)
plt.clim(-100,100)
plt.title('Lagged 30-day Precip anomalies on June 20, 2023', size=18)

# plot figure 18c

var = prec_30d_domain[mats_idx,:] 
var = np.nanmean(var,axis=0)
lat = np.arange(14.75,36.76,0.25)
lon = np.arange(-118.5,-86.74,0.25)
lon, lat = np.meshgrid(lon, lat)

plt.figure(figsize=(10, 8))
m = Basemap(projection='lcc', area_thresh=10000, resolution='l',
            width=3.3E6, height=3.3E6,
            lat_0=25, lon_0=-102)
m.fillcontinents(color='white',lake_color='white',alpha=1,zorder=0)
m.drawstates(linewidth=0.5,linestyle='solid',color='k')
m.drawcountries(linewidth=0.5,linestyle='solid',color='k')
m.drawcoastlines(color='black')
m.pcolormesh(lon, lat, var, latlon=True, cmap=plt.cm.get_cmap('BrBG',20))
cb = plt.colorbar(ticks=np.arange(-100,101,20),extend='max')
cb.ax.tick_params(labelsize=15)
plt.clim(-100,100)
plt.title('Lagged 30-day Precip anomalies on \n top-40 best matched patterns to June 20, 2023', size=18)
plt.show()

# computing analog predictions for all days during May-Aug 2023 (slide 19)

dates = pd.date_range(start='1/1/1940', end='12/31/2023')
dates = dates[~((dates.month == 2) & (dates.day == 29))]
dates = pd.DataFrame(dates)
dates['month'] = pd.to_datetime(dates.iloc[:,0]).dt.month
dates = dates[dates.month.isin([4,5,6,7,8,9])]
dates.reset_index(inplace=True)

day_idx = np.arange(1,184,1)
day_idxs = np.tile(day_idx,83)

# match the patterns

all_mats_idx = np.empty([123,40]) # 123 days, 40 matched patterns for each day
for m in tqdm(range(123)):
    corrs = all_corrs[15219+m,:]

    df1 = pd.DataFrame({'date':dates.iloc[:,1],'month':dates.month,
                        'corr':corrs})  

    df1['year'] = pd.to_datetime(df1.date).dt.year
    df2 = df1[df1.year<=2022]
    df2['day_idx'] = day_idxs
    df3 = df2[df2.day_idx.between(m,30+m)]

    dfx = df3.sort_values(by='corr',ascending=False)
    day_accum = []
    for k in range(df3.shape[0]):
        day = dfx.index.values[k]
        window = list(np.arange(day-4,day+3,1))
        i, j = window[0], window[-1]
        res = any(ele >= i and ele <= j for ele in day_accum)
        if res == 0:
            day_accum.append(day)
    day_accum = np.array(day_accum)

    df5 = df3.loc[day_accum]
    df8 = df5.iloc[:40] # grab top-40

    all_mats_idx[m,:] = df8.index.values

all_mats_idx = all_mats_idx.astype('int32')

# make predictions

actuals = []
preds = []
preds_mlr = []
for m in range(123):
    mats_idx = all_mats_idx[m,:]
    
    zv = z500_amjjas2_z
    z = zv[mats_idx]
    tv = tmax_detrended_z # detrended tmax z-scores to train the models
    t = tv[mats_idx]

    day = zv[15219+m]
    day2 = tmax_z_normed2[15219+m] # actual z-scores (not detrended)
    actuals.append(day2)
    
    dfx = pd.DataFrame({'z':z,'t':t})
    dfx.dropna(inplace=True)
    X = np.array(dfx.z)
    y = np.array(dfx.t)
    
    X = X.reshape(-1,1)
    y = y.reshape(-1,1)
    reg = LinearRegression().fit(X, y)

    new_val = np.array(day)
    new_val = new_val.reshape(-1,1)
    preds.append(np.float64(reg.predict(new_val)))
    
    # MLR
    
    s = sm_z_normed2[mats_idx]
    
    predict_df = pd.DataFrame({'z500':z,
                               'SM':s})
    X = predict_df
    y = pd.DataFrame({'tmax':t})

    model = sm.OLS(y, X).fit()
    
    predict_2023 = np.array([zv[15219+m],sm_z_normed2[15219+m]])
    X_2023 = predict_2023
    preds_mlr.append(np.float64(model.predict(X_2023)))

preds = np.array(preds) # predictions of detrended Tmax z-scores

# first, compute the detrended Tmax values that these z-scores represent
daily_std_detrended = np.load('daily_std_detrended.npy')
std_deg_C = daily_std_detrended[15219:15342]
dep_deg_C = std_deg_C * preds
daily_avg_detrended = np.load('daily_avg_detrended.npy')
pred_deg_C = daily_avg_detrended[15219:15342] + dep_deg_C

# add Tmax trend back in
trend_increments = np.load('tmax_trend_increments.npy')
this_increment = trend_increments[-1]
preds_trend_added = pred_deg_C + this_increment

tmax_normed = np.load('tmax_normed.npy')
tmax_normed_domain = np.nanmean(tmax_normed,axis=1)
observed_tmax = tmax_normed_domain[15219:15342]

# repeat the above workflow for MLR

preds_mlr = np.array(preds_mlr)

# first, compute the detrended Tmax values that these z-scores represent
daily_std_detrended = np.load('daily_std_detrended.npy')
std_deg_C = daily_std_detrended[15219:15342]
dep_deg_C_mlr = std_deg_C * preds_mlr
daily_avg_detrended = np.load('daily_avg_detrended.npy')
pred_deg_C_mlr = daily_avg_detrended[15219:15342] + dep_deg_C_mlr

# add Tmax trend back in
trend_increments = np.load('tmax_trend_increments.npy')
this_increment = trend_increments[-1]
preds_trend_added_mlr = pred_deg_C_mlr + this_increment

# plot figure 19a

fig, ax1 = plt.subplots(figsize=(8,6))
#plt.grid(linewidth=0.5, color='0.5', alpha=0.5, linestyle='-.')
ax1.set_ylabel('Domain-averaged Tmax ($^\circ$C)', fontsize = 16)
ax1.set_yticks(np.arange(24,38.01,2))
ax1.set_ylim(top=38)
ax1.set_ylim(bottom=24)
ax1.set_xticks([0,31,61,92,122])
ax1.set_xticklabels(['May 1','June 1','July 1',
                          'August 1','August 31'], fontsize = 13)
ax1.axvline(x=45,linestyle='--', color='tab:orange', lw=2)
ax1.axvline(x=58,linestyle='--', color='tab:orange', lw=2)
ax1.plot(observed_tmax,'-o', color='firebrick', lw=2)
ax1.plot(pred_deg_C,'-o', color='0.5', lw=2)
ax1.plot(pred_deg_C_mlr,'-o', color='k', lw=2)
ax1.plot(preds_trend_added_mlr,'-o', color='cornflowerblue', lw=2)
ax1.axhline(y=np.mean(observed_tmax),linestyle='--', color='firebrick', lw=2)
ax1.axhline(y=np.mean(pred_deg_C),linestyle='--', color='0.5', lw=2)
ax1.axhline(y=np.mean(pred_deg_C_mlr),linestyle='--', color='k', lw=2)
ax1.axhline(y=np.mean(preds_trend_added_mlr),linestyle='--', color='cornflowerblue', lw=2)
ax1.tick_params(axis='y', labelsize=13)
plt.title('May-August 2023 daily domain-averaged \n Tmax and analog Tmax', fontsize = 20)
plt.savefig('fig_5.pdf',dpi=600)

np.mean(observed_tmax) - np.mean(pred_deg_C)
np.mean(observed_tmax) - np.mean(preds_trend_added)
np.mean(observed_tmax) - np.mean(preds_trend_added_mlr)

# re-plot this on extended y-axis to make legend
fig = plt.figure(figsize=(8,6))
plt.plot(observed_tmax,'-o', color='firebrick', lw=2)
plt.plot(preds_trend_added_mlr,'-o', color='cornflowerblue', lw=2)
plt.plot(preds_trend_added,'-o', color='k', lw=2)
plt.plot(pred_deg_C,'-o', color='0.5', lw=2)
plt.yticks(np.arange(-5000,6001,1000), fontsize=13)
l = plt.legend(['2023 Tmax',
            'Tmax predicted by Z500 and SM + Tmax trend',
            'Tmax predicted by Z500 and SM',
            'Tmax predicted by Z500 only'], 
            loc='lower right', fontsize=15)
colors = ['firebrick','cornflowerblue','k','0.5']
n = -1
for text in l.get_texts():
    n += 1
    text.set_color(colors[n])
plt.savefig('fig_5_legend.pdf',dpi=600)

# plot figure 19b - long term SM time series

sm_z_normed2 = np.load('sm_z_normed2.npy')

# drop April and September
splits = np.split(sm_z_normed2,84)
subset = []
for k in range(84):
    yr = splits[k]
    yr2 = yr[30:153]
    subset.extend(yr2)

subset = np.array(subset)

# get average SM values for each year
splits = np.split(subset,84)
avgs = []
for k in range(84):
    year = splits[k]
    avgs.append(np.nanmean(year))

# compute trend
x = np.arange(84)
y = avgs
intercept = stats.linregress(x,y)[1]
slope = stats.linregress(x,y)[0]
vals1 = []
for j in range(84):
    intercept += slope
    vals1.append(intercept)
(slope*84)/8.4 # 8.4 decades

vals1[-1] - vals1[0]

# permutation test for statistical significance of trend
x = np.arange(84)
slopes1 = np.empty(10000)
for k in range(10000):
    run = np.random.permutation(maxs)
    slopes1[k] = stats.linregress(x,run)[0]
slope_perm = stats.percentileofscore(slopes1, slope)

fig = plt.figure(figsize=(8,6))
plt.plot(avgs,'-o', color='tab:brown')
plt.plot(vals1,'-', color='tab:brown')
#plt.plot(np.polyval(poly, x),'--', color='firebrick')
#plt.grid(linewidth=0.5)
plt.ylim(top=1.7)
plt.xticks(np.arange(0,81,10), ('1940','1950','1960','1970','1980',
                                '1990','2000','2010','2020'), fontsize = 13)
plt.yticks(np.arange(-1.5,1.51,0.5), fontsize=13)
plt.axhline(y=0,color='black',linestyle='--')
plt.ylabel('Domain-averaged SM standardized anomaly', fontsize = 15)
plt.title('Domain-averaged SM standardized anomalies \n during May-August 1940-2023', fontsize = 20)
l = plt.legend(['+0.027 s.d./decade, $\it{p}$ = 0.13)'], loc='upper left', fontsize=15)
for text in l.get_texts():
    text.set_color('tab:brown')


